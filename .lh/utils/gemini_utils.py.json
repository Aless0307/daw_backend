{
    "sourceFile": "utils/gemini_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 6,
            "patches": [
                {
                    "date": 1746282352107,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1746283519989,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,118 @@\n+# Ejemplo: gemini_utils.py (o donde prefieras)\n+\n+import google.generativeai as genai\n+import os\n+import logging\n+import json # Para intentar parsear la respuesta si Gemini devuelve JSON\n+\n+# Configurar logging para este módulo\n+logger = logging.getLogger(__name__)\n+\n+# Configurar API Key (MEJOR desde variables de entorno)\n+GEMINI_API_KEY = \"AIzaSyClAldN4Lvq3HjK1MgogyFdMzitzAqAkXM\"\"\n+if not GEMINI_API_KEY:\n+    logger.warning(\"GEMINI_API_KEY no encontrada en variables de entorno.\")\n+    # Podrías poner una clave por defecto aquí SOLO para pruebas locales MUY CUIDADOSAMENTE\n+    # GEMINI_API_KEY = \"TU_CLAVE_AQUI_CON_CUIDADO\"\n+\n+if GEMINI_API_KEY:\n+    try:\n+        genai.configure(api_key=GEMINI_API_KEY)\n+        logger.info(\"Cliente de Google Generative AI configurado.\")\n+    except Exception as e:\n+        logger.error(f\"Error al configurar Google Generative AI: {e}\")\n+        # Considerar cómo manejar este error globalmente si la API es crítica\n+else:\n+     logger.error(\"No se pudo configurar Google Generative AI: API Key ausente.\")\n+\n+\n+# Configuración del modelo\n+generation_config = {\n+  \"temperature\": 0.7, # Un poco de creatividad pero no demasiada\n+  \"top_p\": 1,\n+  \"top_k\": 1,\n+  \"max_output_tokens\": 2048, # Ajusta según necesidad\n+}\n+\n+safety_settings = [ # Ajusta según necesidad\n+  {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+  {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+  {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+  {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+]\n+\n+async def get_gemini_feedback(problem_text: str, user_answer: str) -> dict | None:\n+    \"\"\"\n+    Llama a la API de Gemini para obtener análisis y calificación.\n+\n+    Returns:\n+        Un diccionario como {\"analysis\": \"...\", \"grade\": X} o None si falla.\n+    \"\"\"\n+    if not GEMINI_API_KEY:\n+        logger.error(\"Intento de llamar a Gemini sin API Key configurada.\")\n+        return None # O lanzar una excepción\n+\n+    try:\n+        model = genai.GenerativeModel(\n+            model_name=\"gemini-1.5-flash\", # O el modelo específico que uses\n+            generation_config=generation_config,\n+            safety_settings=safety_settings\n+        )\n+\n+        # --- INGENIERÍA DEL PROMPT (¡MUY IMPORTANTE!) ---\n+        # Este es un ejemplo básico, necesitarás refinarlo mucho.\n+        prompt = f\"\"\"\n+        Eres un asistente experto en evaluar respuestas a problemas de lógica de programación para estudiantes principiantes.\n+        Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n+        1. Un análisis constructivo y conciso sobre la lógica de la respuesta, indicando puntos fuertes y débiles o sugerencias de mejora. No seas ni muy duro ni muy permisivo.\n+        2. Una calificación numérica entera del 0 al 10, donde 0 es totalmente incorrecto o vacío y 10 es perfecto y bien explicado.\n+\n+        Problema:\n+        \\\"\\\"\\\"\n+        {problem_text}\n+        \\\"\\\"\\\"\n+\n+        Respuesta del Usuario:\n+        \\\"\\\"\\\"\n+        {user_answer}\n+        \\\"\\\"\\\"\n+\n+        IMPORTANTE: Responde únicamente en formato JSON válido con las claves \"analysis\" (string) y \"grade\" (integer). Ejemplo:\n+        {{\"analysis\": \"Tu idea general es correcta, pero la implementación del ciclo anidado no es eficiente para comparar cadenas.\", \"grade\": 6}}\n+        \"\"\"\n+        # --- FIN PROMPT ---\n+\n+        logger.info(\"Generando contenido con Gemini...\")\n+        # Nota: La librería actual podría no ser directamente 'async',\n+        # puede que necesites ejecutar esto en un thread si bloquea.\n+        # Por simplicidad, lo llamamos directamente aquí. Verifica la documentación.\n+        # Si 'generate_content' es síncrona, necesitas envolverla en asyncio.to_thread en el router.\n+        # Asumiendo que puede ser llamada directamente en un endpoint async por ahora:\n+        # (VERIFICAR: Si la librería google-generativeai bloquea, usar asyncio.to_thread en el router)\n+        response = model.generate_content(prompt)\n+\n+        logger.debug(f\"Respuesta cruda de Gemini: {response.text}\")\n+\n+        # Intentar parsear la respuesta como JSON\n+        try:\n+            feedback = json.loads(response.text.strip())\n+            # Validar estructura básica\n+            if isinstance(feedback, dict) and \"analysis\" in feedback and \"grade\" in feedback and isinstance(feedback[\"grade\"], int):\n+                # Validar rango de nota\n+                feedback[\"grade\"] = max(0, min(10, feedback[\"grade\"])) # Asegurar 0-10\n+                logger.info(f\"Feedback parseado de Gemini: Calificación={feedback['grade']}\")\n+                return feedback\n+            else:\n+                logger.error(f\"Respuesta de Gemini no tiene el formato JSON esperado (analysis, grade): {feedback}\")\n+                # Devolver análisis crudo y nota por defecto si falla el parseo/validación\n+                return {\"analysis\": f\"Respuesta de IA (formato inesperado): {response.text}\", \"grade\": 0}\n+\n+        except json.JSONDecodeError:\n+            logger.error(f\"Respuesta de Gemini no es JSON válido: {response.text}\")\n+             # Devolver análisis crudo y nota por defecto si no es JSON\n+            return {\"analysis\": f\"Respuesta de IA (no JSON): {response.text}\", \"grade\": 0}\n+\n+\n+    except Exception as e:\n+        logger.error(f\"Error al llamar a la API de Gemini: {e}\", exc_info=True)\n+        return None # O devolver un feedback de error genérico\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746283530597,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,9 +53,9 @@\n         return None # O lanzar una excepción\n \n     try:\n         model = genai.GenerativeModel(\n-            model_name=\"gemini-1.5-flash\", # O el modelo específico que uses\n+            model_name=\"gemini-2.0-flash\", # O el modelo específico que uses\n             generation_config=generation_config,\n             safety_settings=safety_settings\n         )\n \n@@ -114,123 +114,5 @@\n \n \n     except Exception as e:\n         logger.error(f\"Error al llamar a la API de Gemini: {e}\", exc_info=True)\n-        return None # O devolver un feedback de error genérico\n-# Ejemplo: gemini_utils.py (o donde prefieras)\n-\n-import google.generativeai as genai\n-import os\n-import logging\n-import json # Para intentar parsear la respuesta si Gemini devuelve JSON\n-\n-# Configurar logging para este módulo\n-logger = logging.getLogger(__name__)\n-\n-# Configurar API Key (MEJOR desde variables de entorno)\n-GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") # Asegúrate de tener esta variable de entorno\n-if not GEMINI_API_KEY:\n-    logger.warning(\"GEMINI_API_KEY no encontrada en variables de entorno.\")\n-    # Podrías poner una clave por defecto aquí SOLO para pruebas locales MUY CUIDADOSAMENTE\n-    # GEMINI_API_KEY = \"TU_CLAVE_AQUI_CON_CUIDADO\"\n-\n-if GEMINI_API_KEY:\n-    try:\n-        genai.configure(api_key=GEMINI_API_KEY)\n-        logger.info(\"Cliente de Google Generative AI configurado.\")\n-    except Exception as e:\n-        logger.error(f\"Error al configurar Google Generative AI: {e}\")\n-        # Considerar cómo manejar este error globalmente si la API es crítica\n-else:\n-     logger.error(\"No se pudo configurar Google Generative AI: API Key ausente.\")\n-\n-\n-# Configuración del modelo\n-generation_config = {\n-  \"temperature\": 0.7, # Un poco de creatividad pero no demasiada\n-  \"top_p\": 1,\n-  \"top_k\": 1,\n-  \"max_output_tokens\": 2048, # Ajusta según necesidad\n-}\n-\n-safety_settings = [ # Ajusta según necesidad\n-  {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-  {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-  {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-  {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-]\n-\n-async def get_gemini_feedback(problem_text: str, user_answer: str) -> dict | None:\n-    \"\"\"\n-    Llama a la API de Gemini para obtener análisis y calificación.\n-\n-    Returns:\n-        Un diccionario como {\"analysis\": \"...\", \"grade\": X} o None si falla.\n-    \"\"\"\n-    if not GEMINI_API_KEY:\n-        logger.error(\"Intento de llamar a Gemini sin API Key configurada.\")\n-        return None # O lanzar una excepción\n-\n-    try:\n-        model = genai.GenerativeModel(\n-            model_name=\"gemini-1.5-flash\", # O el modelo específico que uses\n-            generation_config=generation_config,\n-            safety_settings=safety_settings\n-        )\n-\n-        # --- INGENIERÍA DEL PROMPT (¡MUY IMPORTANTE!) ---\n-        # Este es un ejemplo básico, necesitarás refinarlo mucho.\n-        prompt = f\"\"\"\n-        Eres un asistente experto en evaluar respuestas a problemas de lógica de programación para estudiantes principiantes.\n-        Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n-        1. Un análisis constructivo y conciso sobre la lógica de la respuesta, indicando puntos fuertes y débiles o sugerencias de mejora. No seas ni muy duro ni muy permisivo.\n-        2. Una calificación numérica entera del 0 al 10, donde 0 es totalmente incorrecto o vacío y 10 es perfecto y bien explicado.\n-\n-        Problema:\n-        \\\"\\\"\\\"\n-        {problem_text}\n-        \\\"\\\"\\\"\n-\n-        Respuesta del Usuario:\n-        \\\"\\\"\\\"\n-        {user_answer}\n-        \\\"\\\"\\\"\n-\n-        IMPORTANTE: Responde únicamente en formato JSON válido con las claves \"analysis\" (string) y \"grade\" (integer). Ejemplo:\n-        {{\"analysis\": \"Tu idea general es correcta, pero la implementación del ciclo anidado no es eficiente para comparar cadenas.\", \"grade\": 6}}\n-        \"\"\"\n-        # --- FIN PROMPT ---\n-\n-        logger.info(\"Generando contenido con Gemini...\")\n-        # Nota: La librería actual podría no ser directamente 'async',\n-        # puede que necesites ejecutar esto en un thread si bloquea.\n-        # Por simplicidad, lo llamamos directamente aquí. Verifica la documentación.\n-        # Si 'generate_content' es síncrona, necesitas envolverla en asyncio.to_thread en el router.\n-        # Asumiendo que puede ser llamada directamente en un endpoint async por ahora:\n-        # (VERIFICAR: Si la librería google-generativeai bloquea, usar asyncio.to_thread en el router)\n-        response = model.generate_content(prompt)\n-\n-        logger.debug(f\"Respuesta cruda de Gemini: {response.text}\")\n-\n-        # Intentar parsear la respuesta como JSON\n-        try:\n-            feedback = json.loads(response.text.strip())\n-            # Validar estructura básica\n-            if isinstance(feedback, dict) and \"analysis\" in feedback and \"grade\" in feedback and isinstance(feedback[\"grade\"], int):\n-                # Validar rango de nota\n-                feedback[\"grade\"] = max(0, min(10, feedback[\"grade\"])) # Asegurar 0-10\n-                logger.info(f\"Feedback parseado de Gemini: Calificación={feedback['grade']}\")\n-                return feedback\n-            else:\n-                logger.error(f\"Respuesta de Gemini no tiene el formato JSON esperado (analysis, grade): {feedback}\")\n-                # Devolver análisis crudo y nota por defecto si falla el parseo/validación\n-                return {\"analysis\": f\"Respuesta de IA (formato inesperado): {response.text}\", \"grade\": 0}\n-\n-        except json.JSONDecodeError:\n-            logger.error(f\"Respuesta de Gemini no es JSON válido: {response.text}\")\n-             # Devolver análisis crudo y nota por defecto si no es JSON\n-            return {\"analysis\": f\"Respuesta de IA (no JSON): {response.text}\", \"grade\": 0}\n-\n-\n-    except Exception as e:\n-        logger.error(f\"Error al llamar a la API de Gemini: {e}\", exc_info=True)\n         return None # O devolver un feedback de error genérico\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746283553691,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,118 @@\n+# Ejemplo: gemini_utils.py (o donde prefieras)\n+\n+import google.generativeai as genai\n+import os\n+import logging\n+import json # Para intentar parsear la respuesta si Gemini devuelve JSON\n+\n+# Configurar logging para este módulo\n+logger = logging.getLogger(__name__)\n+\n+# Configurar API Key (MEJOR desde variables de entorno)\n+GEMINI_API_KEY = \"AIzaSyClAldN4Lvq3HjK1MgogyFdMzitzAqAkXM\"\n+if not GEMINI_API_KEY:\n+    logger.warning(\"GEMINI_API_KEY no encontrada en variables de entorno.\")\n+    # Podrías poner una clave por defecto aquí SOLO para pruebas locales MUY CUIDADOSAMENTE\n+    # GEMINI_API_KEY = \"TU_CLAVE_AQUI_CON_CUIDADO\"\n+\n+if GEMINI_API_KEY:\n+    try:\n+        genai.configure(api_key=GEMINI_API_KEY)\n+        logger.info(\"Cliente de Google Generative AI configurado.\")\n+    except Exception as e:\n+        logger.error(f\"Error al configurar Google Generative AI: {e}\")\n+        # Considerar cómo manejar este error globalmente si la API es crítica\n+else:\n+     logger.error(\"No se pudo configurar Google Generative AI: API Key ausente.\")\n+\n+\n+# Configuración del modelo\n+generation_config = {\n+  \"temperature\": 0.7, # Un poco de creatividad pero no demasiada\n+  \"top_p\": 1,\n+  \"top_k\": 1,\n+  \"max_output_tokens\": 2048, # Ajusta según necesidad\n+}\n+\n+safety_settings = [ # Ajusta según necesidad\n+  {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+  {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+  {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+  {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n+]\n+\n+async def get_gemini_feedback(problem_text: str, user_answer: str) -> dict | None:\n+    \"\"\"\n+    Llama a la API de Gemini para obtener análisis y calificación.\n+\n+    Returns:\n+        Un diccionario como {\"analysis\": \"...\", \"grade\": X} o None si falla.\n+    \"\"\"\n+    if not GEMINI_API_KEY:\n+        logger.error(\"Intento de llamar a Gemini sin API Key configurada.\")\n+        return None # O lanzar una excepción\n+\n+    try:\n+        model = genai.GenerativeModel(\n+            model_name=\"gemini-2.0-flash\", # O el modelo específico que uses\n+            generation_config=generation_config,\n+            safety_settings=safety_settings\n+        )\n+\n+        # --- INGENIERÍA DEL PROMPT (¡MUY IMPORTANTE!) ---\n+        # Este es un ejemplo básico, necesitarás refinarlo mucho.\n+        prompt = f\"\"\"\n+        Eres un asistente experto en evaluar respuestas a problemas de lógica de programación para estudiantes principiantes.\n+        Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n+        1. Un análisis constructivo y conciso sobre la lógica de la respuesta, indicando puntos fuertes y débiles o sugerencias de mejora. No seas ni muy duro ni muy permisivo.\n+        2. Una calificación numérica entera del 0 al 10, donde 0 es totalmente incorrecto o vacío y 10 es perfecto y bien explicado.\n+\n+        Problema:\n+        \\\"\\\"\\\"\n+        {problem_text}\n+        \\\"\\\"\\\"\n+\n+        Respuesta del Usuario:\n+        \\\"\\\"\\\"\n+        {user_answer}\n+        \\\"\\\"\\\"\n+\n+        IMPORTANTE: Responde únicamente en formato JSON válido con las claves \"analysis\" (string) y \"grade\" (integer). Ejemplo:\n+        {{\"analysis\": \"Tu idea general es correcta, pero la implementación del ciclo anidado no es eficiente para comparar cadenas.\", \"grade\": 6}}\n+        \"\"\"\n+        # --- FIN PROMPT ---\n+\n+        logger.info(\"Generando contenido con Gemini...\")\n+        # Nota: La librería actual podría no ser directamente 'async',\n+        # puede que necesites ejecutar esto en un thread si bloquea.\n+        # Por simplicidad, lo llamamos directamente aquí. Verifica la documentación.\n+        # Si 'generate_content' es síncrona, necesitas envolverla en asyncio.to_thread en el router.\n+        # Asumiendo que puede ser llamada directamente en un endpoint async por ahora:\n+        # (VERIFICAR: Si la librería google-generativeai bloquea, usar asyncio.to_thread en el router)\n+        response = model.generate_content(prompt)\n+\n+        logger.debug(f\"Respuesta cruda de Gemini: {response.text}\")\n+\n+        # Intentar parsear la respuesta como JSON\n+        try:\n+            feedback = json.loads(response.text.strip())\n+            # Validar estructura básica\n+            if isinstance(feedback, dict) and \"analysis\" in feedback and \"grade\" in feedback and isinstance(feedback[\"grade\"], int):\n+                # Validar rango de nota\n+                feedback[\"grade\"] = max(0, min(10, feedback[\"grade\"])) # Asegurar 0-10\n+                logger.info(f\"Feedback parseado de Gemini: Calificación={feedback['grade']}\")\n+                return feedback\n+            else:\n+                logger.error(f\"Respuesta de Gemini no tiene el formato JSON esperado (analysis, grade): {feedback}\")\n+                # Devolver análisis crudo y nota por defecto si falla el parseo/validación\n+                return {\"analysis\": f\"Respuesta de IA (formato inesperado): {response.text}\", \"grade\": 0}\n+\n+        except json.JSONDecodeError:\n+            logger.error(f\"Respuesta de Gemini no es JSON válido: {response.text}\")\n+             # Devolver análisis crudo y nota por defecto si no es JSON\n+            return {\"analysis\": f\"Respuesta de IA (no JSON): {response.text}\", \"grade\": 0}\n+\n+\n+    except Exception as e:\n+        logger.error(f\"Error al llamar a la API de Gemini: {e}\", exc_info=True)\n+        return None # O devolver un feedback de error genérico\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746286355964,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -60,145 +60,34 @@\n         )\n \n         # --- INGENIERÍA DEL PROMPT (¡MUY IMPORTANTE!) ---\n         # Este es un ejemplo básico, necesitarás refinarlo mucho.\n-        prompt = f\"\"\"\n-        Eres un asistente experto en evaluar respuestas a problemas de lógica de programación para estudiantes principiantes.\n-        Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n-        1. Un análisis constructivo y conciso sobre la lógica de la respuesta, indicando puntos fuertes y débiles o sugerencias de mejora. No seas ni muy duro ni muy permisivo.\n-        2. Una calificación numérica entera del 0 al 10, donde 0 es totalmente incorrecto o vacío y 10 es perfecto y bien explicado.\n+# Dentro de get_gemini_feedback en gemini_utils.py\n \n-        Problema:\n-        \\\"\\\"\\\"\n-        {problem_text}\n-        \\\"\\\"\\\"\n+        # ... (inicio de la función) ...\n \n-        Respuesta del Usuario:\n-        \\\"\\\"\\\"\n-        {user_answer}\n-        \\\"\\\"\\\"\n+        prompt = f\"\"\"\n+        Eres un asistente experto y amable que evalúa respuestas a problemas de lógica de programación para estudiantes.\n+        IMPORTANTE: La respuesta del usuario viene de una transcripción de voz, por lo que puede contener pequeños errores (ej. 'ford' en vez de 'for', 'smart.h' en vez de 'math.h', etc.). Sé un poco tolerante con estos errores menores si la lógica subyacente es comprensible. Enfócate en evaluar el *proceso lógico* descrito.\n \n-        IMPORTANTE: Responde únicamente en formato JSON válido con las claves \"analysis\" (string) y \"grade\" (integer). Ejemplo:\n-        {{\"analysis\": \"Tu idea general es correcta, pero la implementación del ciclo anidado no es eficiente para comparar cadenas.\", \"grade\": 6}}\n-        \"\"\"\n-        # --- FIN PROMPT ---\n-\n-        logger.info(\"Generando contenido con Gemini...\")\n-        # Nota: La librería actual podría no ser directamente 'async',\n-        # puede que necesites ejecutar esto en un thread si bloquea.\n-        # Por simplicidad, lo llamamos directamente aquí. Verifica la documentación.\n-        # Si 'generate_content' es síncrona, necesitas envolverla en asyncio.to_thread en el router.\n-        # Asumiendo que puede ser llamada directamente en un endpoint async por ahora:\n-        # (VERIFICAR: Si la librería google-generativeai bloquea, usar asyncio.to_thread en el router)\n-        response = model.generate_content(prompt)\n-\n-        logger.debug(f\"Respuesta cruda de Gemini: {response.text}\")\n-\n-        # Intentar parsear la respuesta como JSON\n-        try:\n-            feedback = json.loads(response.text.strip())\n-            # Validar estructura básica\n-            if isinstance(feedback, dict) and \"analysis\" in feedback and \"grade\" in feedback and isinstance(feedback[\"grade\"], int):\n-                # Validar rango de nota\n-                feedback[\"grade\"] = max(0, min(10, feedback[\"grade\"])) # Asegurar 0-10\n-                logger.info(f\"Feedback parseado de Gemini: Calificación={feedback['grade']}\")\n-                return feedback\n-            else:\n-                logger.error(f\"Respuesta de Gemini no tiene el formato JSON esperado (analysis, grade): {feedback}\")\n-                # Devolver análisis crudo y nota por defecto si falla el parseo/validación\n-                return {\"analysis\": f\"Respuesta de IA (formato inesperado): {response.text}\", \"grade\": 0}\n-\n-        except json.JSONDecodeError:\n-            logger.error(f\"Respuesta de Gemini no es JSON válido: {response.text}\")\n-             # Devolver análisis crudo y nota por defecto si no es JSON\n-            return {\"analysis\": f\"Respuesta de IA (no JSON): {response.text}\", \"grade\": 0}\n-\n-\n-    except Exception as e:\n-        logger.error(f\"Error al llamar a la API de Gemini: {e}\", exc_info=True)\n-        return None # O devolver un feedback de error genérico\n-# Ejemplo: gemini_utils.py (o donde prefieras)\n-\n-import google.generativeai as genai\n-import os\n-import logging\n-import json # Para intentar parsear la respuesta si Gemini devuelve JSON\n-\n-# Configurar logging para este módulo\n-logger = logging.getLogger(__name__)\n-\n-# Configurar API Key (MEJOR desde variables de entorno)\n-GEMINI_API_KEY = \"AIzaSyClAldN4Lvq3HjK1MgogyFdMzitzAqAkXM\"\"\n-if not GEMINI_API_KEY:\n-    logger.warning(\"GEMINI_API_KEY no encontrada en variables de entorno.\")\n-    # Podrías poner una clave por defecto aquí SOLO para pruebas locales MUY CUIDADOSAMENTE\n-    # GEMINI_API_KEY = \"TU_CLAVE_AQUI_CON_CUIDADO\"\n-\n-if GEMINI_API_KEY:\n-    try:\n-        genai.configure(api_key=GEMINI_API_KEY)\n-        logger.info(\"Cliente de Google Generative AI configurado.\")\n-    except Exception as e:\n-        logger.error(f\"Error al configurar Google Generative AI: {e}\")\n-        # Considerar cómo manejar este error globalmente si la API es crítica\n-else:\n-     logger.error(\"No se pudo configurar Google Generative AI: API Key ausente.\")\n-\n-\n-# Configuración del modelo\n-generation_config = {\n-  \"temperature\": 0.7, # Un poco de creatividad pero no demasiada\n-  \"top_p\": 1,\n-  \"top_k\": 1,\n-  \"max_output_tokens\": 2048, # Ajusta según necesidad\n-}\n-\n-safety_settings = [ # Ajusta según necesidad\n-  {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-  {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-  {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-  {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n-]\n-\n-async def get_gemini_feedback(problem_text: str, user_answer: str) -> dict | None:\n-    \"\"\"\n-    Llama a la API de Gemini para obtener análisis y calificación.\n-\n-    Returns:\n-        Un diccionario como {\"analysis\": \"...\", \"grade\": X} o None si falla.\n-    \"\"\"\n-    if not GEMINI_API_KEY:\n-        logger.error(\"Intento de llamar a Gemini sin API Key configurada.\")\n-        return None # O lanzar una excepción\n-\n-    try:\n-        model = genai.GenerativeModel(\n-            model_name=\"gemini-2.0-flash\", # O el modelo específico que uses\n-            generation_config=generation_config,\n-            safety_settings=safety_settings\n-        )\n-\n-        # --- INGENIERÍA DEL PROMPT (¡MUY IMPORTANTE!) ---\n-        # Este es un ejemplo básico, necesitarás refinarlo mucho.\n-        prompt = f\"\"\"\n-        Eres un asistente experto en evaluar respuestas a problemas de lógica de programación para estudiantes principiantes.\n         Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n-        1. Un análisis constructivo y conciso sobre la lógica de la respuesta, indicando puntos fuertes y débiles o sugerencias de mejora. No seas ni muy duro ni muy permisivo.\n-        2. Una calificación numérica entera del 0 al 10, donde 0 es totalmente incorrecto o vacío y 10 es perfecto y bien explicado.\n+        1. Un análisis constructivo y conciso sobre la LÓGICA de la respuesta. Indica si el enfoque es correcto, si es eficiente, si considera casos borde (si aplica), y ofrece sugerencias claras de mejora. Evita juzgar errores menores de sintaxis o nombres si la idea es clara. Estás implementado en una api de speech to text, por lo que puede haber errores al trasncribir la voz a texto, errores de sintaxis, trata de no juzgarlos y de entender lo que realmente quiso decir el usuario.\n+        2. Una calificación numérica ENTERA del 0 al 10. Escala: 0=Vacío/Sin sentido, 1-3=Incorrecto/Muy incompleto, 4-6=Idea básica correcta pero con errores lógicos o muy incompleta, 7-8=Correcto pero mejorable (claridad, eficiencia), 9=Muy bien, casi perfecto, 10=Perfecto, claro, conciso y eficiente.\n \n         Problema:\n         \\\"\\\"\\\"\n         {problem_text}\n         \\\"\\\"\\\"\n \n-        Respuesta del Usuario:\n+        Respuesta del Usuario (transcrita de voz):\n         \\\"\\\"\\\"\n         {user_answer}\n         \\\"\\\"\\\"\n \n-        IMPORTANTE: Responde únicamente en formato JSON válido con las claves \"analysis\" (string) y \"grade\" (integer). Ejemplo:\n-        {{\"analysis\": \"Tu idea general es correcta, pero la implementación del ciclo anidado no es eficiente para comparar cadenas.\", \"grade\": 6}}\n+        RESPUESTA OBLIGATORIA EN FORMATO JSON (SOLO EL JSON, SIN NADA MÁS ANTES O DESPUÉS, SIN MARKDOWN):\n+        {{\"analysis\": \"string con tu análisis aquí\", \"grade\": integer de 0 a 10 aquí}}\n         \"\"\"\n+        # ... (resto de la función: llamada a Gemini, parseo JSON - ¡NECESITAS EL PARSEO CON LIMPIEZA DE MARKDOWN!) ...\n         # --- FIN PROMPT ---\n \n         logger.info(\"Generando contenido con Gemini...\")\n         # Nota: La librería actual podría no ser directamente 'async',\n"
                },
                {
                    "date": 1746286988840,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,77 +1,78 @@\n-# Ejemplo: gemini_utils.py (o donde prefieras)\n+# utils/gemini_utils.py (o donde esté definida la función)\n \n import google.generativeai as genai\n import os\n import logging\n-import json # Para intentar parsear la respuesta si Gemini devuelve JSON\n+import json # Necesario para parsear\n+import re   # Necesario para limpiar la respuesta\n \n # Configurar logging para este módulo\n logger = logging.getLogger(__name__)\n \n # Configurar API Key (MEJOR desde variables de entorno)\n-GEMINI_API_KEY = \"AIzaSyClAldN4Lvq3HjK1MgogyFdMzitzAqAkXM\"\n+GEMINI_API_KEY = \"AIzaSyClAldN4Lvq3HjK1MgogyFdMzitzAqAkXM\" \n if not GEMINI_API_KEY:\n     logger.warning(\"GEMINI_API_KEY no encontrada en variables de entorno.\")\n-    # Podrías poner una clave por defecto aquí SOLO para pruebas locales MUY CUIDADOSAMENTE\n-    # GEMINI_API_KEY = \"TU_CLAVE_AQUI_CON_CUIDADO\"\n+    # Considera añadir una clave por defecto aquí SOLO para desarrollo local\n+    # y con mucho cuidado de no subirla a repositorios públicos.\n+    # GEMINI_API_KEY = \"TU_CLAVE_GEMINI_AQUI\"\n \n if GEMINI_API_KEY:\n     try:\n         genai.configure(api_key=GEMINI_API_KEY)\n         logger.info(\"Cliente de Google Generative AI configurado.\")\n     except Exception as e:\n         logger.error(f\"Error al configurar Google Generative AI: {e}\")\n-        # Considerar cómo manejar este error globalmente si la API es crítica\n else:\n      logger.error(\"No se pudo configurar Google Generative AI: API Key ausente.\")\n \n \n-# Configuración del modelo\n+# Configuración del modelo (Asegúrate de que estas variables estén definidas o pásalas como argumento)\n+# Ejemplo:\n generation_config = {\n-  \"temperature\": 0.7, # Un poco de creatividad pero no demasiada\n+  \"temperature\": 0.6, # Ajustado para ser un poco más determinista\n   \"top_p\": 1,\n   \"top_k\": 1,\n-  \"max_output_tokens\": 2048, # Ajusta según necesidad\n+  \"max_output_tokens\": 1024, # Puedes ajustar esto\n }\n-\n-safety_settings = [ # Ajusta según necesidad\n+safety_settings = [\n   {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n   {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n   {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n   {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n ]\n \n+# --- FUNCIÓN CORREGIDA ---\n async def get_gemini_feedback(problem_text: str, user_answer: str) -> dict | None:\n     \"\"\"\n-    Llama a la API de Gemini para obtener análisis y calificación.\n+    Llama a la API de Gemini para obtener análisis y calificación (0-10).\n+    Limpia la respuesta para extraer JSON antes de parsear.\n \n     Returns:\n-        Un diccionario como {\"analysis\": \"...\", \"grade\": X} o None si falla.\n+        Un diccionario como {\"analysis\": \"...\", \"grade\": X} o un diccionario\n+        de fallback con grade=0 si la llamada o el parseo fallan.\n+        Devuelve None solo si la API Key no está configurada.\n     \"\"\"\n     if not GEMINI_API_KEY:\n         logger.error(\"Intento de llamar a Gemini sin API Key configurada.\")\n-        return None # O lanzar una excepción\n+        # Devolver un diccionario de error consistente es mejor que None aquí\n+        return {\"analysis\": \"Error interno: API Key de Gemini no configurada.\", \"grade\": 0}\n \n     try:\n         model = genai.GenerativeModel(\n-            model_name=\"gemini-2.0-flash\", # O el modelo específico que uses\n+            model_name=\"gemini-1.5-flash\", # Asegúrate que este sea el modelo correcto\n             generation_config=generation_config,\n             safety_settings=safety_settings\n         )\n \n-        # --- INGENIERÍA DEL PROMPT (¡MUY IMPORTANTE!) ---\n-        # Este es un ejemplo básico, necesitarás refinarlo mucho.\n-# Dentro de get_gemini_feedback en gemini_utils.py\n-\n-        # ... (inicio de la función) ...\n-\n+        # --- PROMPT REFINADO ---\n         prompt = f\"\"\"\n         Eres un asistente experto y amable que evalúa respuestas a problemas de lógica de programación para estudiantes.\n         IMPORTANTE: La respuesta del usuario viene de una transcripción de voz, por lo que puede contener pequeños errores (ej. 'ford' en vez de 'for', 'smart.h' en vez de 'math.h', etc.). Sé un poco tolerante con estos errores menores si la lógica subyacente es comprensible. Enfócate en evaluar el *proceso lógico* descrito.\n \n         Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n-        1. Un análisis constructivo y conciso sobre la LÓGICA de la respuesta. Indica si el enfoque es correcto, si es eficiente, si considera casos borde (si aplica), y ofrece sugerencias claras de mejora. Evita juzgar errores menores de sintaxis o nombres si la idea es clara. Estás implementado en una api de speech to text, por lo que puede haber errores al trasncribir la voz a texto, errores de sintaxis, trata de no juzgarlos y de entender lo que realmente quiso decir el usuario.\n+        1. Un análisis constructivo y conciso sobre la LÓGICA de la respuesta. Indica si el enfoque es correcto, si es eficiente, si considera casos borde (si aplica), y ofrece sugerencias claras de mejora. Evita juzgar errores menores de sintaxis o nombres si la idea es clara. Si el usuario menciona una función o concepto clave (como 'len' o 'bucle'), reconócelo positivamente si es apropiado para el problema.\n         2. Una calificación numérica ENTERA del 0 al 10. Escala: 0=Vacío/Sin sentido, 1-3=Incorrecto/Muy incompleto, 4-6=Idea básica correcta pero con errores lógicos o muy incompleta, 7-8=Correcto pero mejorable (claridad, eficiencia), 9=Muy bien, casi perfecto, 10=Perfecto, claro, conciso y eficiente.\n \n         Problema:\n         \\\"\\\"\\\"\n@@ -82,44 +83,83 @@\n         \\\"\\\"\\\"\n         {user_answer}\n         \\\"\\\"\\\"\n \n-        RESPUESTA OBLIGATORIA EN FORMATO JSON (SOLO EL JSON, SIN NADA MÁS ANTES O DESPUÉS, SIN MARKDOWN):\n+        RESPUESTA OBLIGATORIA EN FORMATO JSON VÁLIDO (SOLO EL JSON, SIN NADA MÁS ANTES O DESPUÉS, SIN MARKDOWN ```json ... ```):\n         {{\"analysis\": \"string con tu análisis aquí\", \"grade\": integer de 0 a 10 aquí}}\n         \"\"\"\n-        # ... (resto de la función: llamada a Gemini, parseo JSON - ¡NECESITAS EL PARSEO CON LIMPIEZA DE MARKDOWN!) ...\n         # --- FIN PROMPT ---\n \n         logger.info(\"Generando contenido con Gemini...\")\n-        # Nota: La librería actual podría no ser directamente 'async',\n-        # puede que necesites ejecutar esto en un thread si bloquea.\n-        # Por simplicidad, lo llamamos directamente aquí. Verifica la documentación.\n-        # Si 'generate_content' es síncrona, necesitas envolverla en asyncio.to_thread en el router.\n-        # Asumiendo que puede ser llamada directamente en un endpoint async por ahora:\n-        # (VERIFICAR: Si la librería google-generativeai bloquea, usar asyncio.to_thread en el router)\n+\n+        # --- IMPORTANTE: LLAMADA A GEMINI ---\n+        # La función model.generate_content ES SÍNCRONA en la librería actual.\n+        # Por lo tanto, DEBE ser llamada usando asyncio.to_thread DESDE EL ROUTER (endpoint).\n+        # La firma de ESTA función (get_gemini_feedback) puede ser 'def' normal, no necesita 'async def'.\n+        # Si la dejas como 'async def', la llamada en el router DEBE seguir siendo\n+        # await asyncio.to_thread(get_gemini_feedback, ...) porque la operación interna bloquea.\n+        # Vamos a mantenerla async por ahora, pero recuerda cómo llamarla desde el router.\n         response = model.generate_content(prompt)\n+        # --- FIN LLAMADA A GEMINI ---\n \n-        logger.debug(f\"Respuesta cruda de Gemini: {response.text}\")\n \n-        # Intentar parsear la respuesta como JSON\n-        try:\n-            feedback = json.loads(response.text.strip())\n-            # Validar estructura básica\n-            if isinstance(feedback, dict) and \"analysis\" in feedback and \"grade\" in feedback and isinstance(feedback[\"grade\"], int):\n-                # Validar rango de nota\n-                feedback[\"grade\"] = max(0, min(10, feedback[\"grade\"])) # Asegurar 0-10\n-                logger.info(f\"Feedback parseado de Gemini: Calificación={feedback['grade']}\")\n-                return feedback\n-            else:\n-                logger.error(f\"Respuesta de Gemini no tiene el formato JSON esperado (analysis, grade): {feedback}\")\n-                # Devolver análisis crudo y nota por defecto si falla el parseo/validación\n-                return {\"analysis\": f\"Respuesta de IA (formato inesperado): {response.text}\", \"grade\": 0}\n\\ No newline at end of file\n+        raw_text = response.text.strip()\n+        logger.debug(f\"Respuesta cruda de Gemini recibida: {raw_text}\")\n \n-        except json.JSONDecodeError:\n-            logger.error(f\"Respuesta de Gemini no es JSON válido: {response.text}\")\n-             # Devolver análisis crudo y nota por defecto si no es JSON\n-            return {\"analysis\": f\"Respuesta de IA (no JSON): {response.text}\", \"grade\": 0}\n+        # --- INICIO: LIMPIEZA Y PARSEO JSON ---\n+        json_str = None\n+        # Intentar extraer JSON dentro de ```json ... ``` o solo ``` ... ```\n+        match = re.search(r\"```(?:json)?\\s*({.*?})\\s*```\", raw_text, re.DOTALL | re.IGNORECASE)\n+        if match:\n+            json_str = match.group(1) # Captura el contenido entre llaves {}\n+            logger.info(\"JSON extraído de bloque Markdown.\")\n+        else:\n+            # Si no hay bloques markdown, intentar usar el texto crudo\n+            json_str = raw_text\n+            logger.info(\"No se encontraron bloques Markdown, intentando parsear texto crudo como JSON.\")\n \n+        feedback = None\n+        if json_str:\n+            try:\n+                # Intentar parsear la cadena extraída o cruda\n+                feedback_data = json.loads(json_str)\n \n+                # Validar estructura básica y tipos\n+                if isinstance(feedback_data, dict) and \\\n+                   \"analysis\" in feedback_data and \\\n+                   \"grade\" in feedback_data and \\\n+                   isinstance(feedback_data[\"analysis\"], str) and \\\n+                   isinstance(feedback_data[\"grade\"], (int, float)):\n+\n+                    # Validar y asegurar rango de nota 0-10\n+                    try:\n+                         grade_val = float(feedback_data[\"grade\"]) # Convertir a float por si acaso\n+                         final_grade = round(max(0.0, min(10.0, grade_val))) # Redondear a entero 0-10\n+                    except (ValueError, TypeError):\n+                         logger.error(f\"Valor de 'grade' no es numérico en JSON: {feedback_data.get('grade')}. Usando 0.\")\n+                         final_grade = 0\n+\n+                    feedback = {\n+                        \"analysis\": feedback_data[\"analysis\"].strip(),\n+                        \"grade\": final_grade\n+                    }\n+                    logger.info(f\"Feedback parseado y validado de Gemini: Calificación={feedback['grade']}\")\n+\n+                else:\n+                    logger.error(f\"JSON parseado no tiene la estructura esperada (analysis, grade): {feedback_data}\")\n+                    feedback = {\"analysis\": f\"Respuesta de IA (estructura JSON inesperada): {raw_text}\", \"grade\": 0}\n+\n+            except json.JSONDecodeError:\n+                logger.error(f\"Respuesta de Gemini no es JSON válido (incluso después de limpiar): {json_str}\")\n+                feedback = {\"analysis\": f\"Respuesta de IA (no JSON / error parseo): {raw_text}\", \"grade\": 0}\n+        else:\n+             logger.error(\"No se pudo extraer contenido JSON de la respuesta de Gemini.\")\n+             feedback = {\"analysis\": f\"Respuesta de IA (sin JSON extraíble): {raw_text}\", \"grade\": 0}\n+\n+        return feedback\n+        # --- FIN LIMPIEZA Y PARSEO ---\n+\n     except Exception as e:\n-        logger.error(f\"Error al llamar a la API de Gemini: {e}\", exc_info=True)\n-        return None # O devolver un feedback de error genérico\n+        # Captura cualquier otro error durante la configuración o llamada a Gemini\n+        logger.error(f\"Error general al llamar/configurar Gemini: {e}\", exc_info=True)\n+        # Devolver un feedback de error genérico en lugar de None ayuda al endpoint\n+        return {\"analysis\": f\"Error al contactar al asistente de IA: {str(e)}\", \"grade\": 0}\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746302035774,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,9 +72,9 @@\n \n         Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n         1. Un análisis constructivo y conciso sobre la LÓGICA de la respuesta. Indica si el enfoque es correcto, si es eficiente, si considera casos borde (si aplica), y ofrece sugerencias claras de mejora. Evita juzgar errores menores de sintaxis o nombres si la idea es clara. Si el usuario menciona una función o concepto clave (como 'len' o 'bucle'), reconócelo positivamente si es apropiado para el problema.\n         2. Una calificación numérica ENTERA del 0 al 10. Escala: 0=Vacío/Sin sentido, 1-3=Incorrecto/Muy incompleto, 4-6=Idea básica correcta pero con errores lógicos o muy incompleta, 7-8=Correcto pero mejorable (claridad, eficiencia), 9=Muy bien, casi perfecto, 10=Perfecto, claro, conciso y eficiente.\n-\n+        3. Trata de darle sentido y construir lo que el usuario intentó decir, incluso si no es perfecto. No te limites a señalar errores, sino a ayudar al usuario a mejorar su respuesta. Pero en caso de errores graves, sé claro y directo. Recuerda que el usuario es un estudiante y no un experto. Además de que el usuario está aprendiendo y habla mientras piensa y desarrolla su respuesta. Entonces puede haber trabas, repeticiones, palabras sin sentido, malas transcripciones de la API de speech-to-text, etc. No te limites a señalar errores, sino a ayudar al usuario a mejorar su respuesta, etc. Enfócate en evaluar el proceso lógico descrito.\n         Problema:\n         \\\"\\\"\\\"\n         {problem_text}\n         \\\"\\\"\\\"\n"
                }
            ],
            "date": 1746282352106,
            "name": "Commit-0",
            "content": "# Ejemplo: gemini_utils.py (o donde prefieras)\n\nimport google.generativeai as genai\nimport os\nimport logging\nimport json # Para intentar parsear la respuesta si Gemini devuelve JSON\n\n# Configurar logging para este módulo\nlogger = logging.getLogger(__name__)\n\n# Configurar API Key (MEJOR desde variables de entorno)\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") # Asegúrate de tener esta variable de entorno\nif not GEMINI_API_KEY:\n    logger.warning(\"GEMINI_API_KEY no encontrada en variables de entorno.\")\n    # Podrías poner una clave por defecto aquí SOLO para pruebas locales MUY CUIDADOSAMENTE\n    # GEMINI_API_KEY = \"TU_CLAVE_AQUI_CON_CUIDADO\"\n\nif GEMINI_API_KEY:\n    try:\n        genai.configure(api_key=GEMINI_API_KEY)\n        logger.info(\"Cliente de Google Generative AI configurado.\")\n    except Exception as e:\n        logger.error(f\"Error al configurar Google Generative AI: {e}\")\n        # Considerar cómo manejar este error globalmente si la API es crítica\nelse:\n     logger.error(\"No se pudo configurar Google Generative AI: API Key ausente.\")\n\n\n# Configuración del modelo\ngeneration_config = {\n  \"temperature\": 0.7, # Un poco de creatividad pero no demasiada\n  \"top_p\": 1,\n  \"top_k\": 1,\n  \"max_output_tokens\": 2048, # Ajusta según necesidad\n}\n\nsafety_settings = [ # Ajusta según necesidad\n  {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n  {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n  {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n  {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n]\n\nasync def get_gemini_feedback(problem_text: str, user_answer: str) -> dict | None:\n    \"\"\"\n    Llama a la API de Gemini para obtener análisis y calificación.\n\n    Returns:\n        Un diccionario como {\"analysis\": \"...\", \"grade\": X} o None si falla.\n    \"\"\"\n    if not GEMINI_API_KEY:\n        logger.error(\"Intento de llamar a Gemini sin API Key configurada.\")\n        return None # O lanzar una excepción\n\n    try:\n        model = genai.GenerativeModel(\n            model_name=\"gemini-1.5-flash\", # O el modelo específico que uses\n            generation_config=generation_config,\n            safety_settings=safety_settings\n        )\n\n        # --- INGENIERÍA DEL PROMPT (¡MUY IMPORTANTE!) ---\n        # Este es un ejemplo básico, necesitarás refinarlo mucho.\n        prompt = f\"\"\"\n        Eres un asistente experto en evaluar respuestas a problemas de lógica de programación para estudiantes principiantes.\n        Evalúa la siguiente respuesta de un usuario al problema dado. Proporciona:\n        1. Un análisis constructivo y conciso sobre la lógica de la respuesta, indicando puntos fuertes y débiles o sugerencias de mejora. No seas ni muy duro ni muy permisivo.\n        2. Una calificación numérica entera del 0 al 10, donde 0 es totalmente incorrecto o vacío y 10 es perfecto y bien explicado.\n\n        Problema:\n        \\\"\\\"\\\"\n        {problem_text}\n        \\\"\\\"\\\"\n\n        Respuesta del Usuario:\n        \\\"\\\"\\\"\n        {user_answer}\n        \\\"\\\"\\\"\n\n        IMPORTANTE: Responde únicamente en formato JSON válido con las claves \"analysis\" (string) y \"grade\" (integer). Ejemplo:\n        {{\"analysis\": \"Tu idea general es correcta, pero la implementación del ciclo anidado no es eficiente para comparar cadenas.\", \"grade\": 6}}\n        \"\"\"\n        # --- FIN PROMPT ---\n\n        logger.info(\"Generando contenido con Gemini...\")\n        # Nota: La librería actual podría no ser directamente 'async',\n        # puede que necesites ejecutar esto en un thread si bloquea.\n        # Por simplicidad, lo llamamos directamente aquí. Verifica la documentación.\n        # Si 'generate_content' es síncrona, necesitas envolverla en asyncio.to_thread en el router.\n        # Asumiendo que puede ser llamada directamente en un endpoint async por ahora:\n        # (VERIFICAR: Si la librería google-generativeai bloquea, usar asyncio.to_thread en el router)\n        response = model.generate_content(prompt)\n\n        logger.debug(f\"Respuesta cruda de Gemini: {response.text}\")\n\n        # Intentar parsear la respuesta como JSON\n        try:\n            feedback = json.loads(response.text.strip())\n            # Validar estructura básica\n            if isinstance(feedback, dict) and \"analysis\" in feedback and \"grade\" in feedback and isinstance(feedback[\"grade\"], int):\n                # Validar rango de nota\n                feedback[\"grade\"] = max(0, min(10, feedback[\"grade\"])) # Asegurar 0-10\n                logger.info(f\"Feedback parseado de Gemini: Calificación={feedback['grade']}\")\n                return feedback\n            else:\n                logger.error(f\"Respuesta de Gemini no tiene el formato JSON esperado (analysis, grade): {feedback}\")\n                # Devolver análisis crudo y nota por defecto si falla el parseo/validación\n                return {\"analysis\": f\"Respuesta de IA (formato inesperado): {response.text}\", \"grade\": 0}\n\n        except json.JSONDecodeError:\n            logger.error(f\"Respuesta de Gemini no es JSON válido: {response.text}\")\n             # Devolver análisis crudo y nota por defecto si no es JSON\n            return {\"analysis\": f\"Respuesta de IA (no JSON): {response.text}\", \"grade\": 0}\n\n\n    except Exception as e:\n        logger.error(f\"Error al llamar a la API de Gemini: {e}\", exc_info=True)\n        return None # O devolver un feedback de error genérico"
        }
    ]
}